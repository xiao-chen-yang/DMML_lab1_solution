[["index.html", "STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 1", " STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 1 In week 1, we have studied how to use principal component analysis (PCA) to perform dimension reduction. Before implementing PCA, we should always check that the variables are continuous and perform some exploratory analysis. Some useful codes include: str, skim, summary, cor #for numerical summaries plot, pairs #for graphical summaries Next, we perform PCA by using the command princomp. If the variables have been recorded on different scales or they have very different variances, then it is advisable to base the analysis on the sample correlation matrix; in this case, we set the second argument cor to TRUE. Otherwise, covariance matrix is preferred (no need to include the second argument). my.pca &lt;- princomp(my.data, cor=TRUE) #correlation-based PCA my.pca &lt;- princomp(my.data) #covariance-based PCA To determine the number of principal components to be retained, we could use Proportion of Variation, Kaiser's method and Cattell's method. The former two methods require the standard deviation of each principal component, which can be found by using summary(my.pca) or my.pca$sdev. The last method requires a scree plot, which can be produced by using plot(my.pca). Finally, once PCA is performed, we can interpret the principal components by looking at the loadings (my.pca$loadings). Observations in the new PC coordinate system, i.e. scores, are stored in my.pca$scores. A new observation can be projected into the PC space by using predict(my.pca, new.data). "],["exercise-1-tasks-in-lecture-notes.html", "2 Exercise 1: Tasks in lecture notes", " 2 Exercise 1: Tasks in lecture notes In the lecture note, we looked at the wines dataset and explained how to apply PCA to it. You may re-run the following codes to import the data and perform PCA. wine &lt;- read.csv(&quot;wine.data.csv&quot;) wine.new&lt;-wine[-122,-1] wine.pca&lt;-princomp(wine.new, cor=T) summary(wine.pca) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 ## Standard deviation 2.1861 1.5899 1.1601 0.96171 0.92826 0.80043 0.74398 ## Proportion of Variance 0.3676 0.1945 0.1035 0.07114 0.06628 0.04928 0.04258 ## Cumulative Proportion 0.3676 0.5621 0.6656 0.73675 0.80303 0.85232 0.89489 ## Comp.8 Comp.9 Comp.10 Comp.11 Comp.12 Comp.13 ## Standard deviation 0.57942 0.53979 0.50244 0.48134 0.40738 0.29864 ## Proportion of Variance 0.02583 0.02241 0.01942 0.01782 0.01277 0.00686 ## Cumulative Proportion 0.92072 0.94313 0.96255 0.98037 0.99314 1.00000 Task 2 (in the lecture note) How many components should we have chosen if we wanted 80% of the original variability explained? Answer: components What about 95%? Answer: components Hint Look at the row \"Cumulative Proportion\" and find the column whose value is larger than the required proportion of variance explained. Task 3 Some social scientists use Joliffeâ€™s rule, which says that for a PCA run on correlation, only those PCs with variation above 0.6 should be retained. How many PCs should be retained according to this rule? Answer: components Hint Use wine.pca$sdev to find the standard deviation and thus the variance of each PC. Task 4 Looking at the loadings of the PCA, how would you interpret the third principal component? Task 5 Calculate the first component score for the new observation \\((12,4,3,25,100,2,1,0.4,2,4,1,2,600)\\) by hand (using R as a calculator) Answer: The first component score for the new observation is . Hint Step 1. princomp automatically mean-centres each variable. Therefore, you will have to centre the new observation by taking away the centre vector; the centring vector is stored in wine.pca$center. Step 2. Since we used the correlation matrix and so we were working with standardised data, you will have to scale the resulting centred vector by dividing by the scale vector; the scale vector is stored in wine.pca$scale. Step 3. According to the definition, (PC) scores are the inner product of the new observation (after mean-centring and standardisation) and the first principal component loading vector; the 1st PC loadings are stored in wine.pca$loadings[,1]. Calculate the first component score for the new observation \\((12,4,3,25,100,2,1,0.4,2,4,1,2,600)\\) by using the predict command. Answer: The first component score for the new observation is . "],["exercise-2-employment-in-europe.html", "3 Exercise 2: Employment in Europe", " 3 Exercise 2: Employment in Europe In this example we are going to look at a data set on employment in 26 European countries and perform PCA. The data gives for each of 26 European countries the percentage of the total workforce employed in nine different industries in 1979 (Hand et al, 1994). Variable name Description Agriculture % employed in agriculture Mining % employed in mining Manufacture % employed in manufacturing Power % employed in power supply industries Construction % employed in construction Service % employed in service industries Finance % employed in finance Social % employed in social and personal services Transport % employed in transport &amp; communications employ&lt;-read.table(&quot;eurojob.txt&quot;,header=T,row.names=1);head(employ,4) ## AGRIC MINING MANU POWER CONSTR SERVICE FINANCE SOCIAL TRANS ## Belgium 3.3 0.9 27.6 0.9 8.2 19.1 6.2 26.6 7.2 ## Denmark 9.2 0.1 21.8 0.6 8.3 14.6 6.5 32.2 7.1 ## France 10.8 0.8 27.5 0.9 8.9 16.8 6.0 22.6 5.7 ## WGerm 6.7 1.3 35.8 0.9 7.3 14.4 5.0 22.3 6.1 Task Perform exploratory data analysis (i.e. numerical summaries and some sensible plots) for these data. Comment on the results. Hint For numerical summaries, you could use summary or skim. For plots, you could use pairs. Produce two important numerical summaries for deciding on how to run PCA and to tell how successful it is likely to be. Comment on these. Hint Think about which statistics you will need in order to decide whether PCA might useful for this data, and whether to use covariance-based PCA or correlation-based PCA. Run PCA on the appropriate matrix and look at the output. Hint Use the princomp command to implement PCA. Assuming we are most concerned with preserving information, how many coefficients should we retain if we want to have 90% of the original variability kept? Answer: components Assuming we want to use Cattell's method, how many components would we retain? Hint Use plot on your PCA model to produce a scree plot and look for the bend/elbow. Answer: components Assuming we want to use Kaiser's method, how many components would we retain? Answer: components Assuming we have decided to retain 2 components, is there any useful interpretation to be had for these? Hint Look at the loadings by using $loadings. Produce a scatterplot of the data's scores for the first \\(2\\) PCs and comment. Hint PC scores are stored in$scoresand a scatterplot can be produced by usingplot`. Say we have the following two new observations. obs1&lt;-c(5.1,0.5,32.3,0.8,8.1,16.7,4.3,21.2,6.3) obs2&lt;-c(4.2,0.7,25.4,0.7,9.3,15.0,5.8,31.0,6.9) Calculate their scores on the second principal component. Answer: The second component score for obs1 is and the second component score for obs2 is . Hint You will need to mean-centre and standardise both observations first and then multiply them by the second PC loadings. "],["optional-implementing-pca-using-prcomp.html", "4 Optional: Implementing PCA using prcomp", " 4 Optional: Implementing PCA using prcomp The following code will produce the PCA output given in the tortues videos in the lecture. Try re-running the code using prcomp instead of princomp. (Just start with the data with the outlier removed) #Setting the random generator seed to ensure similar responses when re-running code set.seed(135) ################################## #Reading in and preparing the data ################################## #Open the library ade4 where the data is library(ade4) #Load the tortues dataset data(&quot;tortues&quot;) #Look at the first few lines of the data head(tortues) #Extract the females turtles data into a new dataset called fem.turt fem.turt&lt;-tortues[tortues[,4]==&quot;F&quot;,-4] #Take the log of all the variables in the new dataset log.fem.turt&lt;-log(fem.turt) #Name the variables colnames(log.fem.turt)&lt;-c(&quot;log.length&quot;,&quot;log.width&quot;, &quot;log.breadth&quot;) ############## #Summary Plots ############## #Create a pairsplot of the data pairs(log.fem.turt,pch=20, lower.panel=NULL) #Create a 3-d scatterplot of the data library(lattice) cloud(log.length~log.width*log.breadth,data=log.fem.turt) #Rotate the 3-d scatterplot of the data library(TeachingDemos) #Use your mouse to drag the sliders to change the plot rotate.cloud(log.length~log.width*log.breadth,data=log.fem.turt) #################### #Numerical Summaries #################### #Correlation matrix round(cor(log.fem.turt),2) #Standard deviations apply(log.fem.turt,2,sd) ############################# #Principal Component Analysis ############################# pca.turt&lt;-princomp(log.fem.turt);pca.turt #Change princomp to prcomp and use the help page to find out loadings, scores etc. ###################### #Looking at the scores ###################### head(pca.turt$scores);plot(pca.turt$scores,pch=20) #outlier&lt;-identify(pca.turt$scores) ##################################### #Run PCA on dataset excluding outlier ##################################### pca.turt.new&lt;-princomp(log.fem.turt[-10,]);pca.turt.new #################################### #Deciding on number of PCs to retain #################################### plot(pca.turt.new);summary(pca.turt.new) sd.pca&lt;-summary(pca.turt.new)$sdev tot.var&lt;-sum(sd.pca^2) ave.var&lt;-tot.var/ncol(log.fem.turt) ave.var sd.pca^2&gt;ave.var ########################## #Interpreting the loadings ########################## pca.turt.new$loadings ####################### #Calculating new scores ####################### new.data&lt;-data.frame(log.length=c(4.8),log.width=c(4.7),log.breadth=c(3.9)) predict(pca.turt.new,new.data) Hint The princomp() is replaced with prcomp(). The loadings $loadings are replaced with $rotation. The scores $scores are replaced with $x. "],["solution.html", "5 Solution 5.1 Exercise 1 5.2 Exercise 2 5.3 Optional: Implementing PCA using prcomp", " 5 Solution 5.1 Exercise 1 Task 2. Looking at the Cumulative Proportion line we see: for 80% we would only need 5 components for 95% we would need 10 components. Task 3. According to the new rule, we would obtain 6 components. wine.pca$sdevË†2&gt;0.6 ## logical(0) Task 4. Although there are a large number of positive and negative non-zero loadings, the really dominant ones in terms of size (approximately 0.6) are ash and alcalinity of ash, so this component mainly seems to be interested in ash and its properties in wine. Task 5. To calculate the first component score #By hand using R as a calculator new.x&lt;-matrix(c(12,4,3,25,100,2,1,0.4,2,4,1,2,600),nrow=1) colnames(new.x)&lt;-colnames(wine.new) centre.wine&lt;-wine.pca$center scale.wine&lt;-wine.pca$scale first.load&lt;-wine.pca$loadings[,1] new.x.cent&lt;-new.x-centre.wine new.x.stand&lt;-new.x.cent/scale.wine new.x.stand%*%first.load ## [,1] ## [1,] -1.852 #Using the predict command predict(wine.pca,as.data.frame(new.x)) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## [1,] -1.852 0.1965 -2.91 -0.2902 -0.5356 0.105 -0.4886 0.1793 -1.297 -0.7005 ## Comp.11 Comp.12 Comp.13 ## [1,] 0.4829 0.16 0.7122 We can see the answer by hand is the same as the first element of the predict result. If we were using PCA on the covariance matrix, we would skip the scaling stage (only centering before taking the inner product). 5.2 Exercise 2 The following codes produce some numerical summaries. # summary(employ) library(skimr);library(knitr) my_skim &lt;- skim_with(base = sfl(n = length), numeric = sfl(p0 = NULL, p100 = NULL, hist = NULL)) knit_print(my_skim(employ)) Table 5.1: Data summary Name employ Number of rows 26 Number of columns 9 _______________________ Column type frequency: numeric 9 ________________________ Group variables None Variable type: numeric skim_variable n mean sd p25 p50 p75 AGRIC 26 19.13 15.55 7.70 14.45 23.67 MINING 26 1.25 0.97 0.52 0.95 1.80 MANU 26 27.01 7.01 23.00 27.55 30.20 POWER 26 0.91 0.38 0.60 0.85 1.17 CONSTR 26 8.17 1.65 7.53 8.35 8.97 SERVICE 26 12.96 4.58 9.25 14.40 16.88 FINANCE 26 4.00 2.81 1.22 4.65 5.92 SOCIAL 26 20.02 6.83 16.25 19.65 24.12 TRANS 26 6.55 1.39 5.70 6.70 7.07 Comment: All variables are continuous variables, so it is suitable to apply PCA. The standard deviation vary across variables, which suggest that we should use the correlation matrix in PCA (This will be formally checked shortly). The next step of exploratory analysis is to produce some graphical summaries. Here we could use the pairs plot in R. #Look at the pairs plots for the data pairs(employ,pch=20, lower.panel=NULL) Comment: There are a couple of possible outliers. (We'll leave these for the moment and see if they appear again in our scores plot in the end for whether we need to re-run PCA without them included.) The data doesn't appear to fall into groups. There seem to be some linear relationships between pairs of variables but they are not very strong. It's important to have a look at the correlation and variance of the variables. #Display the correlation matrix to 2 d.p.s #library(matrixcalc) round(cor(employ),2) ## AGRIC MINING MANU POWER CONSTR SERVICE FINANCE SOCIAL TRANS ## AGRIC 1.00 0.04 -0.67 -0.40 -0.54 -0.74 -0.22 -0.75 -0.56 ## MINING 0.04 1.00 0.45 0.41 -0.03 -0.40 -0.44 -0.28 0.16 ## MANU -0.67 0.45 1.00 0.39 0.49 0.20 -0.16 0.15 0.35 ## POWER -0.40 0.41 0.39 1.00 0.06 0.20 0.11 0.13 0.38 ## CONSTR -0.54 -0.03 0.49 0.06 1.00 0.36 0.02 0.16 0.39 ## SERVICE -0.74 -0.40 0.20 0.20 0.36 1.00 0.37 0.57 0.19 ## FINANCE -0.22 -0.44 -0.16 0.11 0.02 0.37 1.00 0.11 -0.25 ## SOCIAL -0.75 -0.28 0.15 0.13 0.16 0.57 0.11 1.00 0.57 ## TRANS -0.56 0.16 0.35 0.38 0.39 0.19 -0.25 0.57 1.00 #Look at the standard deviations of the variables round(sqrt(diag(cov(employ))),1) ## AGRIC MINING MANU POWER CONSTR SERVICE FINANCE SOCIAL TRANS ## 15.5 1.0 7.0 0.4 1.6 4.6 2.8 6.8 1.4 Comment: There are some strong correlation (around -0.7) but a lot of weak (close to zero) correlations as well. It may be possible to achieve some dimension reduction here but not a lot. The variation for some variables is much bigger than for others, therefore we should use the correlation matrix and not the covariance matrix in PCA. We should use princomp with the argument cor=T to perform PCA on the correlation matrix. employ.pca&lt;-princomp(employ,cor=T) employ.pca ## Call: ## princomp(x = employ, cor = T) ## ## Standard deviations: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 ## 1.867392 1.459511 1.048312 0.997238 0.737033 0.619215 0.475136 0.369851 ## Comp.9 ## 0.006755 ## ## 9 variables and 26 observations. The summary function can give us information about how many components we should keep. summary(employ.pca) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 ## Standard deviation 1.8674 1.4595 1.0483 0.9972 0.73703 0.6192 0.47514 ## Proportion of Variance 0.3875 0.2367 0.1221 0.1105 0.06036 0.0426 0.02508 ## Cumulative Proportion 0.3875 0.6241 0.7463 0.8568 0.91711 0.9597 0.98480 ## Comp.8 Comp.9 ## Standard deviation 0.3699 6.755e-03 ## Proportion of Variance 0.0152 5.069e-06 ## Cumulative Proportion 1.0000 1.000e+00 Looking at the Cumulative Proportion line, it is clear that we need to keep 5 components to retain 90% variability. For Cattell's method we will need to produce a scree plot. plot(employ.pca) There are some different possibilities here, depending on how you read the graph. Suggesting 2, 4 or 6 components seems reasonable as that's where we see sudden drops/leveling off in variation. For Kaiser's method we need to look at finding the average eigenvalue to discover which set of components have variation above it that we will retain. employ.pca$sdev^2&gt;1 ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 ## TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE Here we have cheated a bit, since we know the average variance is going to be \\(1\\) whenever we use the correlation matrix. We can see that we would retain the first \\(3\\) components in this case. To interpret principal components, we look at the loadings from each component. employ.pca$loadings ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 ## AGRIC 0.524 0.213 0.153 0.806 ## MINING 0.618 -0.201 -0.164 -0.101 -0.726 ## MANU -0.347 0.355 -0.150 -0.346 -0.385 -0.288 0.479 0.126 0.366 ## POWER -0.256 0.261 -0.561 0.393 0.295 0.357 0.256 -0.341 ## CONSTR -0.325 0.153 -0.668 0.472 0.130 -0.221 -0.356 ## SERVICE -0.379 -0.350 -0.115 -0.284 0.615 -0.229 0.388 0.238 ## FINANCE -0.454 -0.587 0.280 -0.526 -0.187 0.174 0.145 ## SOCIAL -0.387 -0.222 0.312 0.412 -0.220 -0.263 -0.191 -0.506 0.351 ## TRANS -0.367 0.203 0.375 0.314 0.513 -0.124 0.545 ## ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 ## SS loadings 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ## Proportion Var 0.111 0.111 0.111 0.111 0.111 0.111 0.111 0.111 0.111 ## Cumulative Var 0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000 Component \\(1\\) seems to be the difference between the average of manufacturing, power, construction, service, social and transporation industries, and the agricultural industry. So this new variable will distinguish between countries with agricultural economies and those with industrial economies. Component \\(2\\) seems to be the difference between the average of mining, manufacturing, power and transportation industries, and the average of service, finance and social industries. So this new variable distinguishes between contries with relatively large and relatively small service sectors. The scatterplot of scores for the first \\(2\\) PCs is produced as follows. employ.scores2&lt;-as.data.frame(employ.pca$scores[,1:2]) plot(employ.scores2$Comp.1,employ.scores2$Comp.2,xlab=&quot;Comp.1&quot;, ylab=&quot;Comp.2&quot;) There definitely seems to be an issue with at least one outlier. It would be worth identifying and removing it/them and re-running PCA to see if it affects the results. We can calculate the scores using the following code #By hand using R as a calculator centre.employ&lt;-employ.pca$center scale.employ&lt;-employ.pca$scale second.load&lt;-employ.pca$loadings[,2] obs1.cent&lt;-obs1-centre.employ obs1.stand&lt;-obs1.cent/scale.employ obs1.stand%*%second.load ## [,1] ## [1,] -0.7608 obs2.cent&lt;-obs2-centre.employ obs2.stand&lt;-obs2.cent/scale.employ obs2.stand%*%second.load ## [,1] ## [1,] -1.373 #Using the predict command newdata&lt;-rbind(obs1,obs2);colnames(newdata)&lt;-colnames(employ) new.data&lt;-as.data.frame(newdata); new.data.scores&lt;-predict(employ.pca,new.data) new.data.scores[, 1:6] ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 ## obs1 -0.989 -0.7608 -0.01662 -0.46329 -0.80507 0.03885 ## obs2 -1.468 -1.3729 0.70027 0.02475 -0.03189 -0.66059 5.3 Optional: Implementing PCA using prcomp The following code uses prcomp to analyse the turtle data. #Setting the random generator seed to ensure similar responses when re-running code set.seed(135) ############################# #Principal Component Analysis ############################# pca.turt.2&lt;-prcomp(log.fem.turt[-10,]);pca.turt.2 #################################### #Deciding on number of PCs to retain #################################### plot(pca.turt.2);summary(pca.turt.2) sd.pca&lt;-summary(pca.turt.2)$sdev tot.var&lt;-sum(sd.pca^2) ave.var&lt;-tot.var/ncol(log.fem.turt) ave.var sd.pca^2&gt;ave.var #######################################IT############## #Interpreting the loadings and calculating new scores ##################################################### pca.turt.2$rotation #loading matrix # pca.turt.2$x #PC scores new.data&lt;-data.frame(log.length=c(4.8),log.width=c(4.7),log.breadth=c(3.9)) predict(pca.turt.2,new.data) The PCA results using princomp and prcomp are not too different. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
